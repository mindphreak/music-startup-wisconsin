a <- 23
help.start()
help.start()
?mean
?mean
?rnorm
x <- rnorm(50)
y <- rnorm(x)
plot(x,y)
plot(x,y)
0
y <- 20
plot(x,y)
x <- rnorm(50)
y <- rnorm(x)
plot(x,y)
x <- rnorm(50)
y <- rnorm(x)
plot(x,y)
View(x)
print(x)
boxplot(x,y)
boxplot(x,y)
plot(x,y)
hist(x,y)
hist(x,y)
hist(x)
view(x)
view(x)
?boxplot
x <- rnorm(50)
y <- rnorm(x)
boxplot(x)
x <- rnorm(50)
y <- rnorm(x)
boxplot(y)
boxplot(y)
?rnorm
rm(y)
x <- rnorm(50,2)
y <- rnorm(x)
boxplot(y)
x <- rnorm(50,2)
y <- rnorm(x)
boxplot(x)
x <- rnorm(50,100)
y <- rnorm(x)
boxplot(x)
x <- rnorm(50,2)
y <- rnorm(x)
boxplot(x)
x <- rnorm(50,2,0)
y <- rnorm(x)
boxplot(x)
x<-c(10.4, 5.6, 3.1, 6.4, 21.7)
1/x
x*2
x<-x*2
x<-x/2
y<-x<7 & x>3
view(y)
View(x)
print(x)
y<-x>mean(x)
print(y)
mean(x)
y<-x>mean(x)
print(y)
?sd
sd(x)
?Quotes
\a
\\a
?c
?paste
paste(c("X","Y"), 1:20, sep="+")
paste(c("X"), 1:20, sep="+")
paste(c("X","Y","T"), 1:20, sep="+")
paste(c("X","Y","X", "O"), 1:20, sep="+")
?paste
paste(c("X","Y","X", "O"), -6:20, sep="+")
x<-c(-5:1, 0/0)
y<-x[!is.na(x)]
z<-x[2:4]
paste(c("X","Y","X", "O"), -6:20, sep="\a")
paste(c("X","Y","X", "O"), -6:20, sep="\\a")
paste(c("X","Y","X", "O"), -6:20, sep="\n")
PRINT("HEY")
print("HEY")
a <- 2
a <- 2
b =3
4 -> c
total = total + 1000
total = 1000
total = total + 1000
print(total)
total
6%%4
6 %% 4
6 %/% 4
107 %/% 4
107 / 4
celciusInput <- readline("Temperature in Celcius:")
fahrResults <- ((9/5)*celciusNumeric)+32
fahrString <- sprintf("%.2f", fahrResults)
message <- paste("Temperature in fahrenheit : ", fahrString, sep="")
print(message)
print(message)
celciusInput <- readline("Temperature in Celcius:")
celciusNumeric <- as.numeric(celciusInput)
fahrResults <- ((9/5)*celciusNumeric)+32
fahrString <- sprintf("%.2f", fahrResults)
message <- paste("Temperature in fahrenheit : ", fahrString, sep="")
print(message)
class(a)
typeof(5/9)
?names
name = c("a", "b", "c")
age = c(2, 4, 6)
mark = c(50, 60, 90)
dat = data.frame(name, age, mark)
summary(dat)
myfile = file("Cities.txt", open = "w")
getwd()
myfile
write(2,myfile, append = TRUE)
write(5,myfile, sep="\n")
myfile = file("Cities.txt", open = "r")
myLines = readLines(myfile)
myfile = file("Cities.txt", open = "r")
myLines = readLines(myfile)
total = 0
for(i in 1:length(myLines)) {
total= total + i
}
avg = total/length(myLines)
myfile
avg
length(myLines)
total = 0
for(i in 1:length(myLines)) {
total= total + myLines[i]
}
avg = total/length(myLines)
myfile = file("Cities.txt", open = "r")
myLines = readLines(myfile)
total = 0
for(i in 1:length(myLines)) {
total= total + myLines[i]
}
myLines[1]
total
myfile = file("Cities.txt", open = "r")
myLines = readLines(myfile)
total = 0
for(i in 1:length(myLines)) {
total= total + as.numeric(myLines[i])
}
avg = total/length(myLines)
?writeResult
install.packages("htmltab")
install.packages("rjson")
library("htmltab")
url <- "https://en.wikipedia.org/wiki/Game_of_Thrones"
seasons <- htmltab(doc = url, which = 2)
print(seasons)
View(seasons)
rownames(seasons)
rownames(seasons) <- c(1:8)
head(seasons)
?head
head(seasons)
view(head(seasons))
criticResponses <- htmltab(doc = url, which = 3)
View(criticResponses)
rownames(criticResponses) <- c(1:8)
seasonFactor <- as.factor(criticResponses$Season)
criticResponses$Season <- as.factor(criticResponses$Season)
str(criticResponses)
GoTWikipedia <- merge(seasons, criticResponses, by="Season")
View(GoTWikipedia)
seasons$Season <- as.factor(seasons$Season)
GoTWikipedia <- merge(seasons, criticResponses, by="Season")
seasons$Season <- as.factor(criticResponses$Season)
GoTWikipedia <- merge(seasons, criticResponses, by="Season")
fakeSeasons <- c(8:100)
fakeText <- rep("fake", length(fakeSeasons))
library(MASS)
library(ISLR)
install.packages(ISLR)
install.packages("ISLR")
library(ISLR)
data("Boston")
names(Boston)
table(Boston)
?Boston
View(Boston)
?names
?Boston
lm.fit(medv~lstat, data)
attach(Boston)
lm.fit(medv~lstat, data)
lm.fit(medv~lstat, data)
attach(Boston)
lm.fit(medv~lstat, data=Boston)
attach(Boston)
lm.fit(medv~lstat)
lm.fit(medv~lstat, data=Boston)
attach(Boston)
attach(Boston)
lm.fit = lm(medv~lstat)
View(lm.fit)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
resid(lm.fit)
terms(lm.fit)
confint(lm.fit)
?confint
mean(Boston$medv)
mean(Boston$lstat)
revisionTime <- c(4,9,10,14,4,7,12,22,1,17)
library(ISLR)
data("Hitters")
Hitters <- na.omit(Hitters)
x <-model.matrix(Salary~.,Hitters)[,-1]
y <-Hitters$Salary
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
View(Hitters)
View(x)
View(x)
View(Hitters)
z <-model.matrix(Salary~.,Hitters)
View(z)
rnorm(1)
rnorm(1)
set.seed(1)
rnorm(1)
set.seed(1)
rnorm(1)
set.seed(1)
rnorm(1)
set.seed(8)
rnorm(1)
set.seed(8)
rnorm(1)
rnorm(1)
library(pls)
install.packages(pls)
install.packages("pls")
library("pls")
set.seed(2)
pcr.fit = pcr(Salary~., data = Hitters, scale = TRUE)
pcr.fit = pcr(Salary~., data = Hitters, scale = TRUE, validation ="CV")
View(pcr.fit)
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")
validationplot(pcr.fit, val.type = "RMSEP")
set.seed(1)
pcr.fit = pcr(Salary~., data = Hitters, subset=train, scale=TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MMSEP")
validationplot(pcr.fit, val.type = "MSEP")
pcr.pred = predict(pcr.fit,[test,], ncomp=7)
pcr.pred = predict(pcr.fit,x[test,], ncomp=7)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y~x, scale=TRUE, ncomp=7)
install.packages("women")
data("women")
force(women)
plot(women$height, women$weight)
fit2 <- lm(weight~height+I(height^2)), data=women)
fit2 <- lm(weight~height+I(height^2), data=women)
summary(fit2)
par(mfrow=c(1,1))
plot(fit2)
fit3 <- lm(weighyt~height+I(height^2), data=women[-c(13,15),])
fit3 <- lm(weight~height+I(height^2), data=women[-c(13,15),])
summary(fit3)
plot(fit3)
?abline
abline(fit2)
?women
par(mfrow=c(2,2))
plot(fit3)
par(mfrow=c(3,3))
plot(fit3)
par(mfrow=c(6,6))
plot(fit3)
par(mfrow=c(1,4))
plot(fit3)
par(mfrow=c(2,2))
plot(fit3)
view(women)
view(women)
print(women)
carSpeeds <- read.csv(file = 'D:\Downloads\Firefox downloads\ozone.data.csv')
carSpeeds <- read.csv(file = 'D:\\Downloads\\Firefox downloads\\ozone.data.csv')
ozoneData <- read.csv(file = 'D:\\Downloads\\Firefox downloads\\ozone.data.csv')
plot(ozoneData$ozone,ozoneData$temp)
plot(ozoneData$ozone,ozoneData$)
plot(ozoneData$ozone,ozoneData$wind)
head(ozoneData)
pairs(ozoneData)
pairs(ozoneData, panel = panel.smooth)
?pairs
model1 <- lm(ozone~temp*wind*rad+I(rad^2)+I(temp^2)+I(wind^2), data = ozoneData)
summary(model1)
model2 <- update(model1,~.-temp:wind:rad)
summary(model2)
model3<-update(model2,~.-wind:rad)
summary(model3)
model4<-update(model2,~.-temp:wind)
model5<-update(model4,~.-I(rad^2))
summary(model5)
model6<-update(model5,~.-temp:rad)
summary(model6)
?I
library(fpp2)
install.packages("fpp2"")
install.packages("fpp2")
install.packages(fpp2)
install.packages("fpp2")
library(fpp2)
data("books")
force(books)
autoplot(books)
plot(books)
autoplot(books)
fcast1 <- ses(books[,"Hardcover"], h=4)
fcast2 <- ses(books[,"Paperback"], h=4)
fcast1
fcast2
autolayer(fcast2, Series="Paperback", PI=FALSE)
autoplot(books) +
autolayer(fcast1, Series="Hardcover", PI=FALSE)
autolayer(fcast2, Series="Paperback", PI=FALSE)
accuracy(fcast1)
accuracy(fcast2)
fcast4 <- holt(books[,"Paperback", h=4])
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
fcast3 <- holt(books[,"Hardcover", h=4])
fcast4 <- holt(books[,"Paperback", h=4])
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
fcast3 <- holt(books[,"Hardcover", h=4])
fcast3 <- holt(books[,"Hardcover"], h=4)
fcast4 <- holt(books[,"Paperback"], h=4)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=TRUE)
autolayer(fcast4, Series="Paperback", PI=TRUE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
accuracy(fcast3)
accuracy(fcast4)
library(fpp2)
data(pigs)
library(tseries)
data(pigs)
pigs
autoplot(pigs)
fcast.ses <- ses(pigs, h=4)
fcast.ses
summary(fcast.ses)
bicoal
autoplot(bicoal)
ggtsdisplay(bicoal)
coalfit<=arima(bicoal, order=c(4,0,0))
coalfit<-arima(bicoal, order=c(4,0,0))
forecast(coalfit, h=3)
autoplot(bicoal)
ggtsdisplay(bicoal)
checkresiduals(coalfit)
coalfit<-arima(bicoal, order=c(4,0,0))
checkresiduals(coalfit)
wmurders
autoplot(wmurders)
ggtsdisplay(wmurders)
murderfit<-arima(wmurders, order=c(4,0,0))
forecast(murderfit)
murdiff<- diff(wmurders)
ggtsdisplay((murdiff))
murdiff2<- diff(murdiff)
ggtsdisplay((murdiff2))
adf.test(murdiff)
adf.test(murdiff2)
?ndiffs
murderfit<-arima(wmurders, order=c(0,1,2))
forecast(murderfit)
checkresiduals(wmurders)
murderfit<-arima(murdiff, order=c(0,1,2))
checkresiduals(wmurders)
murderfit<-arima(murdiff, order=c(2,1,0))
murderfit<-arima(murdiff, order=c(0,1,2))
checkresiduals(murderfit)
murderfit2<-arima(murdiff, order=c(2,1,0))
checkresiduals(murderfit2)
#Importing files
song60s <- read.csv("dataset-of-60s.csv")
song70s <- read.csv("dataset-of-70s.csv")
song80s <- read.csv("dataset-of-80s.csv")
song90s <- read.csv("dataset-of-90s.csv")
song00s <- read.csv("dataset-of-00s.csv")
song10s <- read.csv("dataset-of-10s.csv")
#Combining all
songsAll <- rbind(song60s, song70s, song80s, song90s, song00s, song10s)
songsAll <- songsAll[,4:19]
classifier = svm(formula = target ~ ., data = training_set, type = 'C-classification', kernel = 'linear')
#Importing files
song60s <- read.csv("dataset-of-60s.csv")
song70s <- read.csv("dataset-of-70s.csv")
song80s <- read.csv("dataset-of-80s.csv")
song90s <- read.csv("dataset-of-90s.csv")
song00s <- read.csv("dataset-of-00s.csv")
setwd("D:/Data Playground/Academics/Data Mining/Datasets/Spotify Hit Predictor")
#Importing files
song60s <- read.csv("dataset-of-60s.csv")
song70s <- read.csv("dataset-of-70s.csv")
song80s <- read.csv("dataset-of-80s.csv")
song90s <- read.csv("dataset-of-90s.csv")
song00s <- read.csv("dataset-of-00s.csv")
song10s <- read.csv("dataset-of-10s.csv")
#Combining all
songsAll <- rbind(song60s, song70s, song80s, song90s, song00s, song10s)
songsAll <- songsAll[,4:19]
#install.packages("caTools")
library("caTools")
set.seed(123)
split = sample.split(songsAll$target, SplitRatio = 0.8)
training_set = subset(songsAll, split == TRUE)
test_set = subset(songsAll, split == FALSE)
#Feature scaling
training_set[,1:4] = scale(training_set[,1:4])
training_set[,6:15] = scale(training_set[,6:15])
test_set[,1:4] = scale(test_set[,1:4])
test_set[,6:15] = scale(test_set[,6:15])
library(e1071)
classifier = svm(formula = target ~ ., data = training_set, type = 'C-classification', kernel = 'linear')
y_pred = predict(classifier, newdata = test_set[-16])
#Making confusion matrix
cm = table(test_set[, 16], y_pred)
cm
#Applying K Fold Cross Validation
#install.packages("caret")
library(caret)
folds = createFolds(training_set$target,k = 10)
cv = lapply(folds, function(x) {
training_fold = training_set[-x, ]
test_fold = test_set[x,]
classifier = glm(formula = target ~ ., family = binomial, data = training_fold)
prob_pred = predict(classifier, type = "response", newdata = test_fold[-16])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
cm = table(test_fold[, 16], y_pred)
accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
return (accuracy)
})
accuracy = mean(as.numeric(cv))
accuracy
