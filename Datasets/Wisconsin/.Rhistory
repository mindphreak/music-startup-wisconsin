paste(c("X","Y","T"), 1:20, sep="+")
paste(c("X","Y","X", "O"), 1:20, sep="+")
?paste
paste(c("X","Y","X", "O"), -6:20, sep="+")
x<-c(-5:1, 0/0)
y<-x[!is.na(x)]
z<-x[2:4]
paste(c("X","Y","X", "O"), -6:20, sep="\a")
paste(c("X","Y","X", "O"), -6:20, sep="\\a")
paste(c("X","Y","X", "O"), -6:20, sep="\n")
PRINT("HEY")
print("HEY")
a <- 2
a <- 2
b =3
4 -> c
total = total + 1000
total = 1000
total = total + 1000
print(total)
total
6%%4
6 %% 4
6 %/% 4
107 %/% 4
107 / 4
celciusInput <- readline("Temperature in Celcius:")
fahrResults <- ((9/5)*celciusNumeric)+32
fahrString <- sprintf("%.2f", fahrResults)
message <- paste("Temperature in fahrenheit : ", fahrString, sep="")
print(message)
print(message)
celciusInput <- readline("Temperature in Celcius:")
celciusNumeric <- as.numeric(celciusInput)
fahrResults <- ((9/5)*celciusNumeric)+32
fahrString <- sprintf("%.2f", fahrResults)
message <- paste("Temperature in fahrenheit : ", fahrString, sep="")
print(message)
class(a)
typeof(5/9)
?names
name = c("a", "b", "c")
age = c(2, 4, 6)
mark = c(50, 60, 90)
dat = data.frame(name, age, mark)
summary(dat)
myfile = file("Cities.txt", open = "w")
getwd()
myfile
write(2,myfile, append = TRUE)
write(5,myfile, sep="\n")
myfile = file("Cities.txt", open = "r")
myLines = readLines(myfile)
myfile = file("Cities.txt", open = "r")
myLines = readLines(myfile)
total = 0
for(i in 1:length(myLines)) {
total= total + i
}
avg = total/length(myLines)
myfile
avg
length(myLines)
total = 0
for(i in 1:length(myLines)) {
total= total + myLines[i]
}
avg = total/length(myLines)
myfile = file("Cities.txt", open = "r")
myLines = readLines(myfile)
total = 0
for(i in 1:length(myLines)) {
total= total + myLines[i]
}
myLines[1]
total
myfile = file("Cities.txt", open = "r")
myLines = readLines(myfile)
total = 0
for(i in 1:length(myLines)) {
total= total + as.numeric(myLines[i])
}
avg = total/length(myLines)
?writeResult
install.packages("htmltab")
install.packages("rjson")
library("htmltab")
url <- "https://en.wikipedia.org/wiki/Game_of_Thrones"
seasons <- htmltab(doc = url, which = 2)
print(seasons)
View(seasons)
rownames(seasons)
rownames(seasons) <- c(1:8)
head(seasons)
?head
head(seasons)
view(head(seasons))
criticResponses <- htmltab(doc = url, which = 3)
View(criticResponses)
rownames(criticResponses) <- c(1:8)
seasonFactor <- as.factor(criticResponses$Season)
criticResponses$Season <- as.factor(criticResponses$Season)
str(criticResponses)
GoTWikipedia <- merge(seasons, criticResponses, by="Season")
View(GoTWikipedia)
seasons$Season <- as.factor(seasons$Season)
GoTWikipedia <- merge(seasons, criticResponses, by="Season")
seasons$Season <- as.factor(criticResponses$Season)
GoTWikipedia <- merge(seasons, criticResponses, by="Season")
fakeSeasons <- c(8:100)
fakeText <- rep("fake", length(fakeSeasons))
library(MASS)
library(ISLR)
install.packages(ISLR)
install.packages("ISLR")
library(ISLR)
data("Boston")
names(Boston)
table(Boston)
?Boston
View(Boston)
?names
?Boston
lm.fit(medv~lstat, data)
attach(Boston)
lm.fit(medv~lstat, data)
lm.fit(medv~lstat, data)
attach(Boston)
lm.fit(medv~lstat, data=Boston)
attach(Boston)
lm.fit(medv~lstat)
lm.fit(medv~lstat, data=Boston)
attach(Boston)
attach(Boston)
lm.fit = lm(medv~lstat)
View(lm.fit)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
resid(lm.fit)
terms(lm.fit)
confint(lm.fit)
?confint
mean(Boston$medv)
mean(Boston$lstat)
revisionTime <- c(4,9,10,14,4,7,12,22,1,17)
library(ISLR)
data("Hitters")
Hitters <- na.omit(Hitters)
x <-model.matrix(Salary~.,Hitters)[,-1]
y <-Hitters$Salary
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
View(Hitters)
View(x)
View(x)
View(Hitters)
z <-model.matrix(Salary~.,Hitters)
View(z)
rnorm(1)
rnorm(1)
set.seed(1)
rnorm(1)
set.seed(1)
rnorm(1)
set.seed(1)
rnorm(1)
set.seed(8)
rnorm(1)
set.seed(8)
rnorm(1)
rnorm(1)
library(pls)
install.packages(pls)
install.packages("pls")
library("pls")
set.seed(2)
pcr.fit = pcr(Salary~., data = Hitters, scale = TRUE)
pcr.fit = pcr(Salary~., data = Hitters, scale = TRUE, validation ="CV")
View(pcr.fit)
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")
validationplot(pcr.fit, val.type = "RMSEP")
set.seed(1)
pcr.fit = pcr(Salary~., data = Hitters, subset=train, scale=TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MMSEP")
validationplot(pcr.fit, val.type = "MSEP")
pcr.pred = predict(pcr.fit,[test,], ncomp=7)
pcr.pred = predict(pcr.fit,x[test,], ncomp=7)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y~x, scale=TRUE, ncomp=7)
install.packages("women")
data("women")
force(women)
plot(women$height, women$weight)
fit2 <- lm(weight~height+I(height^2)), data=women)
fit2 <- lm(weight~height+I(height^2), data=women)
summary(fit2)
par(mfrow=c(1,1))
plot(fit2)
fit3 <- lm(weighyt~height+I(height^2), data=women[-c(13,15),])
fit3 <- lm(weight~height+I(height^2), data=women[-c(13,15),])
summary(fit3)
plot(fit3)
?abline
abline(fit2)
?women
par(mfrow=c(2,2))
plot(fit3)
par(mfrow=c(3,3))
plot(fit3)
par(mfrow=c(6,6))
plot(fit3)
par(mfrow=c(1,4))
plot(fit3)
par(mfrow=c(2,2))
plot(fit3)
view(women)
view(women)
print(women)
carSpeeds <- read.csv(file = 'D:\Downloads\Firefox downloads\ozone.data.csv')
carSpeeds <- read.csv(file = 'D:\\Downloads\\Firefox downloads\\ozone.data.csv')
ozoneData <- read.csv(file = 'D:\\Downloads\\Firefox downloads\\ozone.data.csv')
plot(ozoneData$ozone,ozoneData$temp)
plot(ozoneData$ozone,ozoneData$)
plot(ozoneData$ozone,ozoneData$wind)
head(ozoneData)
pairs(ozoneData)
pairs(ozoneData, panel = panel.smooth)
?pairs
model1 <- lm(ozone~temp*wind*rad+I(rad^2)+I(temp^2)+I(wind^2), data = ozoneData)
summary(model1)
model2 <- update(model1,~.-temp:wind:rad)
summary(model2)
model3<-update(model2,~.-wind:rad)
summary(model3)
model4<-update(model2,~.-temp:wind)
model5<-update(model4,~.-I(rad^2))
summary(model5)
model6<-update(model5,~.-temp:rad)
summary(model6)
?I
library(fpp2)
install.packages("fpp2"")
install.packages("fpp2")
install.packages(fpp2)
install.packages("fpp2")
library(fpp2)
data("books")
force(books)
autoplot(books)
plot(books)
autoplot(books)
fcast1 <- ses(books[,"Hardcover"], h=4)
fcast2 <- ses(books[,"Paperback"], h=4)
fcast1
fcast2
autolayer(fcast2, Series="Paperback", PI=FALSE)
autoplot(books) +
autolayer(fcast1, Series="Hardcover", PI=FALSE)
autolayer(fcast2, Series="Paperback", PI=FALSE)
accuracy(fcast1)
accuracy(fcast2)
fcast4 <- holt(books[,"Paperback", h=4])
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
fcast3 <- holt(books[,"Hardcover", h=4])
fcast4 <- holt(books[,"Paperback", h=4])
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
fcast3 <- holt(books[,"Hardcover", h=4])
fcast3 <- holt(books[,"Hardcover"], h=4)
fcast4 <- holt(books[,"Paperback"], h=4)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=TRUE)
autolayer(fcast4, Series="Paperback", PI=TRUE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
accuracy(fcast3)
accuracy(fcast4)
library(fpp2)
data(pigs)
library(tseries)
data(pigs)
pigs
autoplot(pigs)
fcast.ses <- ses(pigs, h=4)
fcast.ses
summary(fcast.ses)
bicoal
autoplot(bicoal)
ggtsdisplay(bicoal)
coalfit<=arima(bicoal, order=c(4,0,0))
coalfit<-arima(bicoal, order=c(4,0,0))
forecast(coalfit, h=3)
autoplot(bicoal)
ggtsdisplay(bicoal)
checkresiduals(coalfit)
coalfit<-arima(bicoal, order=c(4,0,0))
checkresiduals(coalfit)
wmurders
autoplot(wmurders)
ggtsdisplay(wmurders)
murderfit<-arima(wmurders, order=c(4,0,0))
forecast(murderfit)
murdiff<- diff(wmurders)
ggtsdisplay((murdiff))
murdiff2<- diff(murdiff)
ggtsdisplay((murdiff2))
adf.test(murdiff)
adf.test(murdiff2)
?ndiffs
murderfit<-arima(wmurders, order=c(0,1,2))
forecast(murderfit)
checkresiduals(wmurders)
murderfit<-arima(murdiff, order=c(0,1,2))
checkresiduals(wmurders)
murderfit<-arima(murdiff, order=c(2,1,0))
murderfit<-arima(murdiff, order=c(0,1,2))
checkresiduals(murderfit)
murderfit2<-arima(murdiff, order=c(2,1,0))
checkresiduals(murderfit2)
#Importing files
song60s <- read.csv("dataset-of-60s.csv")
song70s <- read.csv("dataset-of-70s.csv")
song80s <- read.csv("dataset-of-80s.csv")
song90s <- read.csv("dataset-of-90s.csv")
song00s <- read.csv("dataset-of-00s.csv")
song10s <- read.csv("dataset-of-10s.csv")
#Combining all
songsAll <- rbind(song60s, song70s, song80s, song90s, song00s, song10s)
songsAll <- songsAll[,4:19]
install.packages("vcd")
library(vcd)
setwd("D:/Data Playground/Academics/Data Mining/Datasets/Spotify Hit Predictor")
# Music hit prediction using logistic regression and naive bayes.
library(dplyr)
#Importing files
song60s <- read.csv("dataset-of-60s.csv")
song70s <- read.csv("dataset-of-70s.csv")
song80s <- read.csv("dataset-of-80s.csv")
song90s <- read.csv("dataset-of-90s.csv")
song00s <- read.csv("dataset-of-00s.csv")
song10s <- read.csv("dataset-of-10s.csv")
#Combining all
songsAll <- rbind(song60s, song70s, song80s, song90s, song00s, song10s)
songsAll <- songsAll[,4:19]
songsAll[, 'mode'] = factor(songsAll[,'mode'])
songsAll[, 'target'] = factor(songsAll[,'target'])
#install.packages("caTools")
library("caTools")
set.seed(123)
split = sample.split(songsAll$target, SplitRatio = 0.8)
training_set = subset(songsAll, split == TRUE)
test_set = subset(songsAll, split == FALSE)
#Feature scaling
training_set[,1:4] = scale(training_set[,1:4])
training_set[,6:15] = scale(training_set[,6:15])
test_set[,1:4] = scale(test_set[,1:4])
test_set[,6:15] = scale(test_set[,6:15])
classifier = naiveBayes( x = training_set[-16], y = training_set$target)
y_pred = predict(classifier, newdata = test_set[-16])
library(e1071)
classifier = naiveBayes( x = training_set[-16], y = training_set$target)
y_pred = predict(classifier, newdata = test_set[-16])
cm = table(test_set[,16], y_pred)
library(vcd)
kappa(cm)
cm
Kappa(cm)
classifier = glm(formula = target ~ ., family = binomial, data = training_fold)
classifier = glm(formula = target ~ ., family = binomial, data = training_set)
prob_pred = predict(classifier, type = "response", newdata = test_fold[-16])
prob_pred = predict(classifier, type = "response", newdata = test_set[-16])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
cm = table(test_fold[, 16], y_pred)
cm = table(test_set[, 16], y_pred)
Kappa(cm)
setwd("D:/Data Playground/Academics/Data Mining/Datasets/Startups")
crunchbase_data <- read.csv("investments_VC.csv")
#Removing NAs
startups <- na.omit(crunchbase_data)
table(startups$status)
#trimming leading and trailing white space from status
startups$status <- trimws(startups$status, which = c("both"))
library(dplyr)
startups <- startups %>% select("seed", "venture", "debt_financing", "angel", "funding_rounds", "round_A", "round_B", "round_C", "status")
startups <- startups[!(startups$status == "closed" | startups$status==""),]
startups <- mutate_all(startups, list(~na_if(.,"")))
startups <- na.omit(startups)
table(startups$status)
startups$status = factor(startups$status, levels = c('acquired', 'operating'), labels = c(0,1))
#install.packages("ROSE")
library(ROSE)
install.packages("DMwR")
library(DMwR)
startups.smote <- SMOTE(status~., data = startups, k = 5)
table(startups.smote$status)
table(startups.rose$status)
startups.rose <- ROSE(status ~ ., data = startups, seed = 1)$data
table(startups.rose$status)
startups.smote <- SMOTE(status~., data = startups, k = 5, perc.over = 17840)
table(startups.smote$status)
startups.smote <- SMOTE(status~., data = startups, k = 5, perc.over = 100)
table(startups.smote$status)
startups.smote <- SMOTE(status~., data = startups, k = 5, perc.over = 200)
table(startups.smote$status)
startups.smote <- SMOTE(status~., data = startups, k = 5, perc.over = 150)
table(startups.smote$status)
startups.smote <- SMOTE(status~., data = startups, k = 5, perc.over = 200)
table(startups.smote$status)
train.index <- createDataPartition(startups.smote$status, p = .7, list = FALSE)
training_set <- startups.smote[ train.index,]
library(caret)
train.index <- createDataPartition(startups.smote$status, p = .7, list = FALSE)
training_set <- startups.smote[ train.index,]
testing_set  <- startups.smote[-train.index,]
table(training_set$status)
table(testing_set$status)
#Feature scaling
training_set[,1:8] = scale(training_set[,1:8])
testing_set[,1:8] = scale(testing_set[,1:8])
#Decision tree
library(rpart)
tree.rose <- rpart(status ~ ., data = training_set)
pred.tree.rose <- predict(tree.rose, newdata = testing_set[-9], type = "class")
roc.curve(testing_set$status, pred.tree.rose)
cm = table(testing_set[, 9], y_pred)
classifier = glm(formula = status ~ ., family = binomial, data = training_set)
summary(classifier)
#Predicting test results
prob_pred = predict(classifier, type = "response", newdata = testing_set[,1:8])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
cm = table(testing_set[, 9], y_pred)
cm
library(e1071)
classifier = svm(formula = status ~ ., data = startups.smote, type = 'C-classification', kernel = 'linear')
y_pred = predict(classifier, newdata = testing_set[-9])
cm = table(test_set[, 9], y_pred)
confusionMatrix(testing_set[, 9], y_pred)
classifier = svm(formula = status ~ ., data = training_set, type = 'C-classification', kernel = 'linear')
y_pred = predict(classifier, newdata = testing_set[-9])
cm = table(test_set[, 9], y_pred)
cm = table(testing_set[, 9], y_pred)
confusionMatrix(testing_set[, 9], y_pred)
setwd("D:/Data Playground/Academics/Data Mining/Datasets/Startups")
wls <- read.csv("wls_subset - highschool score and iq.csv")
setwd("D:/Data Playground/Academics/Data Mining/Datasets/Wisconsin")
library(GGally)
install.packages("GGally")
#Removing NAs
library(dplyr)
data <- mutate_all(wls, list(~na_if(.,"-3")))
data <- data[,c(4:15,3,2)]
data = data[!is.na(data$hsrscorq),]
library(mice)
imp <- mice(data, m=5, maxit = 50, method = 'pmm', seed = 500)
wls <- read.csv("wls_subset - highschool score and iq.csv")
#Removing NAs
library(dplyr)
data <- mutate_all(wls, list(~na_if(.,"-3")))
data <- data[,c(4:15,3,2)]
data = data[!is.na(data$hsrscorq),]
library(mice)
imp <- mice(data, m=5, maxit = 50, method = 'pmm', seed = 500)
summary(imp)
completeData <- complete(imp,1)
summary(completeData)
View(completeData)
X<-completeData[,1:13]
library(GGally)
ggpairs(X)
warning()
warnings()
wls <- read.csv("wls_subset - highschool score and iq.csv")
#Removing NAs
library(dplyr)
data <- mutate_all(wls, list(~na_if(.,"-3")))
data <- data[,c(4:15,3,2)]
data = data[!is.na(data$hsrscorq),]
library(mice)
imp <- mice(data, m=5, maxit = 50, method = 'pmm', seed = 500)
completeData <- complete(imp,1)
completeData <- na.omit(completeData)
X<-completeData[,1:13]
ggpairs(X)
library("caTools")
set.seed(123)
split = sample.split(completeData$hsrscorq, SplitRatio = 0.8)
training_set = subset(completeData, split == TRUE)
test_set = subset(completeData, split == FALSE)
#Feature scaling
training_set[,1:14] = scale(training_set[,1:14])
test_set[,1:14] = scale(test_set[,1:14])
regressor = lm(formula = hsrscorq~., data = training_set )
summary(regressor)
car::vif(regressor)
alias(regressor)
