view(head(seasons))
criticResponses <- htmltab(doc = url, which = 3)
View(criticResponses)
rownames(criticResponses) <- c(1:8)
seasonFactor <- as.factor(criticResponses$Season)
criticResponses$Season <- as.factor(criticResponses$Season)
str(criticResponses)
GoTWikipedia <- merge(seasons, criticResponses, by="Season")
View(GoTWikipedia)
seasons$Season <- as.factor(seasons$Season)
GoTWikipedia <- merge(seasons, criticResponses, by="Season")
seasons$Season <- as.factor(criticResponses$Season)
GoTWikipedia <- merge(seasons, criticResponses, by="Season")
fakeSeasons <- c(8:100)
fakeText <- rep("fake", length(fakeSeasons))
library(MASS)
library(ISLR)
install.packages(ISLR)
install.packages("ISLR")
library(ISLR)
data("Boston")
names(Boston)
table(Boston)
?Boston
View(Boston)
?names
?Boston
lm.fit(medv~lstat, data)
attach(Boston)
lm.fit(medv~lstat, data)
lm.fit(medv~lstat, data)
attach(Boston)
lm.fit(medv~lstat, data=Boston)
attach(Boston)
lm.fit(medv~lstat)
lm.fit(medv~lstat, data=Boston)
attach(Boston)
attach(Boston)
lm.fit = lm(medv~lstat)
View(lm.fit)
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
resid(lm.fit)
terms(lm.fit)
confint(lm.fit)
?confint
mean(Boston$medv)
mean(Boston$lstat)
revisionTime <- c(4,9,10,14,4,7,12,22,1,17)
library(ISLR)
data("Hitters")
Hitters <- na.omit(Hitters)
x <-model.matrix(Salary~.,Hitters)[,-1]
y <-Hitters$Salary
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
View(Hitters)
View(x)
View(x)
View(Hitters)
z <-model.matrix(Salary~.,Hitters)
View(z)
rnorm(1)
rnorm(1)
set.seed(1)
rnorm(1)
set.seed(1)
rnorm(1)
set.seed(1)
rnorm(1)
set.seed(8)
rnorm(1)
set.seed(8)
rnorm(1)
rnorm(1)
library(pls)
install.packages(pls)
install.packages("pls")
library("pls")
set.seed(2)
pcr.fit = pcr(Salary~., data = Hitters, scale = TRUE)
pcr.fit = pcr(Salary~., data = Hitters, scale = TRUE, validation ="CV")
View(pcr.fit)
summary(pcr.fit)
validationplot(pcr.fit, val.type = "MSEP")
validationplot(pcr.fit, val.type = "RMSEP")
set.seed(1)
pcr.fit = pcr(Salary~., data = Hitters, subset=train, scale=TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MMSEP")
validationplot(pcr.fit, val.type = "MSEP")
pcr.pred = predict(pcr.fit,[test,], ncomp=7)
pcr.pred = predict(pcr.fit,x[test,], ncomp=7)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y~x, scale=TRUE, ncomp=7)
install.packages("women")
data("women")
force(women)
plot(women$height, women$weight)
fit2 <- lm(weight~height+I(height^2)), data=women)
fit2 <- lm(weight~height+I(height^2), data=women)
summary(fit2)
par(mfrow=c(1,1))
plot(fit2)
fit3 <- lm(weighyt~height+I(height^2), data=women[-c(13,15),])
fit3 <- lm(weight~height+I(height^2), data=women[-c(13,15),])
summary(fit3)
plot(fit3)
?abline
abline(fit2)
?women
par(mfrow=c(2,2))
plot(fit3)
par(mfrow=c(3,3))
plot(fit3)
par(mfrow=c(6,6))
plot(fit3)
par(mfrow=c(1,4))
plot(fit3)
par(mfrow=c(2,2))
plot(fit3)
view(women)
view(women)
print(women)
carSpeeds <- read.csv(file = 'D:\Downloads\Firefox downloads\ozone.data.csv')
carSpeeds <- read.csv(file = 'D:\\Downloads\\Firefox downloads\\ozone.data.csv')
ozoneData <- read.csv(file = 'D:\\Downloads\\Firefox downloads\\ozone.data.csv')
plot(ozoneData$ozone,ozoneData$temp)
plot(ozoneData$ozone,ozoneData$)
plot(ozoneData$ozone,ozoneData$wind)
head(ozoneData)
pairs(ozoneData)
pairs(ozoneData, panel = panel.smooth)
?pairs
model1 <- lm(ozone~temp*wind*rad+I(rad^2)+I(temp^2)+I(wind^2), data = ozoneData)
summary(model1)
model2 <- update(model1,~.-temp:wind:rad)
summary(model2)
model3<-update(model2,~.-wind:rad)
summary(model3)
model4<-update(model2,~.-temp:wind)
model5<-update(model4,~.-I(rad^2))
summary(model5)
model6<-update(model5,~.-temp:rad)
summary(model6)
?I
library(fpp2)
install.packages("fpp2"")
install.packages("fpp2")
install.packages(fpp2)
install.packages("fpp2")
library(fpp2)
data("books")
force(books)
autoplot(books)
plot(books)
autoplot(books)
fcast1 <- ses(books[,"Hardcover"], h=4)
fcast2 <- ses(books[,"Paperback"], h=4)
fcast1
fcast2
autolayer(fcast2, Series="Paperback", PI=FALSE)
autoplot(books) +
autolayer(fcast1, Series="Hardcover", PI=FALSE)
autolayer(fcast2, Series="Paperback", PI=FALSE)
accuracy(fcast1)
accuracy(fcast2)
fcast4 <- holt(books[,"Paperback", h=4])
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
fcast3 <- holt(books[,"Hardcover", h=4])
fcast4 <- holt(books[,"Paperback", h=4])
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
fcast3 <- holt(books[,"Hardcover", h=4])
fcast3 <- holt(books[,"Hardcover"], h=4)
fcast4 <- holt(books[,"Paperback"], h=4)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=TRUE)
autolayer(fcast4, Series="Paperback", PI=TRUE)
autoplot(books) +
autolayer(fcast3, Series="Hardcover", PI=FALSE)
autolayer(fcast4, Series="Paperback", PI=FALSE)
accuracy(fcast3)
accuracy(fcast4)
library(fpp2)
data(pigs)
library(tseries)
data(pigs)
pigs
autoplot(pigs)
fcast.ses <- ses(pigs, h=4)
fcast.ses
summary(fcast.ses)
bicoal
autoplot(bicoal)
ggtsdisplay(bicoal)
coalfit<=arima(bicoal, order=c(4,0,0))
coalfit<-arima(bicoal, order=c(4,0,0))
forecast(coalfit, h=3)
autoplot(bicoal)
ggtsdisplay(bicoal)
checkresiduals(coalfit)
coalfit<-arima(bicoal, order=c(4,0,0))
checkresiduals(coalfit)
wmurders
autoplot(wmurders)
ggtsdisplay(wmurders)
murderfit<-arima(wmurders, order=c(4,0,0))
forecast(murderfit)
murdiff<- diff(wmurders)
ggtsdisplay((murdiff))
murdiff2<- diff(murdiff)
ggtsdisplay((murdiff2))
adf.test(murdiff)
adf.test(murdiff2)
?ndiffs
murderfit<-arima(wmurders, order=c(0,1,2))
forecast(murderfit)
checkresiduals(wmurders)
murderfit<-arima(murdiff, order=c(0,1,2))
checkresiduals(wmurders)
murderfit<-arima(murdiff, order=c(2,1,0))
murderfit<-arima(murdiff, order=c(0,1,2))
checkresiduals(murderfit)
murderfit2<-arima(murdiff, order=c(2,1,0))
checkresiduals(murderfit2)
#Importing files
song60s <- read.csv("dataset-of-60s.csv")
song70s <- read.csv("dataset-of-70s.csv")
song80s <- read.csv("dataset-of-80s.csv")
song90s <- read.csv("dataset-of-90s.csv")
song00s <- read.csv("dataset-of-00s.csv")
song10s <- read.csv("dataset-of-10s.csv")
#Combining all
songsAll <- rbind(song60s, song70s, song80s, song90s, song00s, song10s)
songsAll <- songsAll[,4:19]
setwd("D:/Data Playground/Academics/Data Mining/Datasets/Wisconsin")
corr <- cor(completeData)
wls <- read.csv("wls_subset - highschool score and iq.csv")
#Removing NAs
library(dplyr)
data <- mutate_all(wls, list(~na_if(.,"-3")))
data <- data[,c(4:15,3,2)]
data = data[!is.na(data$hsrscorq),]
library(mice)
imp <- mice(data, m=5, maxit = 50, method = 'pmm', seed = 500)
completeData <- complete(imp,1)
completeData = na.omit(completeData)
corr <- cor(completeData)
View(corr)
library(car)
pca <- prcomp(completeData[,c(1:13)], center = TRUE, scale. = TRUE)
summary(pca)
pca$x
plot(pca$x[1], pca$x[2])
plot(pca$x[,1], pca$x[,2])
plot(pca$x[,2], pca$x[,3])
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100,1)
barplot(pca.var.per)
## now make a fancy looking plot that shows the PCs and the variation:
library(ggplot2)
pca.data <- data.frame(Sample=rownames(pca$x),
X=pca$x[,1],
Y=pca$x[,2])
pca.data
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("My PCA Graph")
## get the name of the top 10 measurements (genes) that contribute
## most to pc1.
loading_scores <- pca$rotation[,1]
gene_scores <- abs(loading_scores) ## get the magnitudes
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes ## show the names of the top 10 genes
pca$rotation[top_10_genes,1] ## show the scores (and +/- sign)
View(completeData)
completeData <- completeData %>% select("ghnrs_j", "ghnrs_f", "gwiiq_j", "gwiiq_f",  "hsrscorq")
library("caTools")
set.seed(123)
split = sample.split(completeData$hsrscorq, SplitRatio = 0.8)
training_set = subset(completeData, split == TRUE)
test_set = subset(completeData, split == FALSE)
#Feature scaling
training_set[,1:14] = scale(training_set[,1:14])
test_set[,1:14] = scale(test_set[,1:14])
install.packages("randomForest")
library(randomForest)
set.seed(123)
regressor = randomForest( formula = hsrscorq~., data= training_set)
plot(regressor)
y_pred =predict(regressor, data)
plot(data)
points(data$hsrscorq, y_pred, col = "red", pch=16)
X<-completeData[,1:13]
regressor = lm(formula = hsrscorq~., data = training_set )
summary(regressor)
setwd("D:/Data Playground/Academics/Data Mining/Datasets/Spotify Hit Predictor")
setwd("D:/Data Playground/Academics/Data Mining/Datasets/Spotify Hit Predictor")
# Music hit prediction using logistic regression and naive bayes.
library(dplyr)
#Importing files
song60s <- read.csv("dataset-of-60s.csv")
song70s <- read.csv("dataset-of-70s.csv")
song80s <- read.csv("dataset-of-80s.csv")
song90s <- read.csv("dataset-of-90s.csv")
song00s <- read.csv("dataset-of-00s.csv")
song10s <- read.csv("dataset-of-10s.csv")
#Combining all
songsAll <- rbind(song60s, song70s, song80s, song90s, song00s, song10s)
songsAll <- songsAll[,4:19]
songsAll[, 'mode'] = factor(songsAll[,'mode'])
songsAll[, 'target'] = factor(songsAll[,'target'])
#install.packages("caTools")
library("caTools")
set.seed(123)
split = sample.split(songsAll$target, SplitRatio = 0.8)
training_set = subset(songsAll, split == TRUE)
test_set = subset(songsAll, split == FALSE)
#Feature scaling
training_set[,1:4] = scale(training_set[,1:4])
training_set[,6:15] = scale(training_set[,6:15])
test_set[,1:4] = scale(test_set[,1:4])
test_set[,6:15] = scale(test_set[,6:15])
library(e1071)
classifier = naiveBayes( x = training_set[-16], y = training_set$target)
y_pred = predict(classifier, newdata = test_set[-16])
cm = table(test_set[,16], y_pred)
cm
classifier = glm(formula = target ~ ., family = binomial, data = training_set)
#PCA
pca <- prcomp(songsAll[,c(1:4,6:15)], center = TRUE, scale. = TRUE)
summary(pca)
plot(pca$x[,1], pca$x[,2]])
plot(pca$x[,1], pca$x[,2])
CMetrics(cm)
CMetrics <- function(cm) {
cm
accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy
}
CMetrics(cm)
CMetrics <- function(cm) {
cm
accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy
recall = (cm[1,1])/cm[1,1]+cm[1,2])
recall
}
recall = ((cm[1,1])/cm[1,1]+cm[1,2])
CMetrics <- function(cm) {
cm
accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy
recall = ((cm[1,1])/cm[1,1]+cm[1,2])
recall
}
CMetrics(cm)
CMetrics <- function(cm) {
accuracy = (cm[1,1] + cm[2,2])/(cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy
recall = ((cm[1,1])/cm[1,1]+cm[1,2])
recall
}
cm = table(test_set[, 16], y_pred)
CMetrics(cm)
#Random Forest Classification
library(randomForest)
classifier = randomForest(x = training_set[-16], y = training_set$target, ntree = 120)
y_pred = predict(classifier, newdata = test_set[-16])
cm = table(test_set[, 3], y_pred)
cm = table(test_set[, 16], y_pred)
cm
classifier = glm(formula = target ~ ., family = binomial, data = training_set)
prob_pred = predict(classifier, type = "response", newdata = test_set[-16])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
cm = table(test_set[, 16], y_pred)
cm
classifier = randomForest(x = training_set[-16], y = training_set$target, ntree = 120)
y_pred = predict(classifier, newdata = test_set[-16])
cm = table(test_set[, 16], y_pred)
cm
confusionMatrix(test_set[, 16], y_pred)
#Logistic Regression and  K Fold Cross Validation
#install.packages("caret")
library(caret)
confusionMatrix(test_set[, 16], y_pred)
setwd("D:/Data Playground/Academics/Data Mining/Datasets/Startups")
crunchbase_data <- read.csv("investments_VC.csv")
#Removing NAs
library(VIM)
url_plot <- aggr(data, col=c('green', 'red'), combined = TRUE,
numbers=TRUE, sortVars=TRUE,
labels=names(crunchbase_data), cex.axis=.7,
gap=3, ylab=c("Missing data","Pattern"))
crunchbase_data <- mutate_all(crunchbase_data, list(~na_if(.,"")))
url_plot <- aggr(data, col=c('green', 'red'), combined = TRUE,
numbers=TRUE, sortVars=TRUE,
labels=names(crunchbase_data), cex.axis=.7,
gap=3, ylab=c("Missing data","Pattern"))
View(crunchbase_data)
crunchbase_data <- read.csv("investments_VC.csv")
crunchbase_data <- crunchbase_data %>% select("seed", "venture", "debt_financing", "angel", "funding_rounds", "round_A", "round_B", "round_C", "status")
crunchbase_data <- mutate_all(crunchbase_data, list(~na_if(.,"")))
url_plot <- aggr(data, col=c('green', 'red'), combined = TRUE,
numbers=TRUE, sortVars=TRUE,
labels=names(crunchbase_data), cex.axis=.7,
gap=3, ylab=c("Missing data","Pattern"))
url_plot <- aggr(crunchbase_data, col=c('green', 'red'), combined = TRUE,
numbers=TRUE, sortVars=TRUE,
labels=names(crunchbase_data), cex.axis=.7,
gap=3, ylab=c("Missing data","Pattern"))
startups <- na.omit(crunchbase_data)
View(startups)
#trimming leading and trailing white space from status
startups$status <- trimws(startups$status, which = c("both"))
table(startups$status)
startups <- startups[!(startups$status == "closed" | startups$status==""),]
table(startups$status)
startups$status = factor(startups$status, levels = c('acquired', 'operating'), labels = c(0,1))
glimpse(startups)
count(startups)
count(startups, vars = "seed")
table(startups$round_A)
pca <- prcomp(startups[,c(1:8)], center = TRUE, scale. = TRUE)
summary(pca)
#Splitting into training and test
library(caret)
train.index <- createDataPartition(startups$status, p = .7, list = FALSE)
training_set <- startups.rose[ train.index,]
training_set <- startups[ train.index,]
testing_set  <- startups[-train.index,]
table(training_set$status)
table(testing_set$status)
#install.packages("ROSE")
library(ROSE)
startups.rose <- ROSE(status ~ ., data = training_set, seed = 1)$data
table(startups.rose$status)
#SMOTE sampling
#install.packages("DMwR")
library(DMwR)
startups.smote <- SMOTE(status~., data = training_set, k = 10, perc.over = 200)
table(startups.smote$status)
#Feature scaling
training_set[,1:8] = scale(training_set[,1:8])
testing_set[,1:8] = scale(testing_set[,1:8])
#install.packages("ROSE")
library(ROSE)
startups.rose <- ROSE(status ~ ., data = training_set, seed = 1)$data
table(startups.rose$status)
#SMOTE sampling
#install.packages("DMwR")
library(DMwR)
startups.smote <- SMOTE(status~., data = training_set, k = 10, perc.over = 200)
table(startups.smote$status)
classifier = glm(formula = status ~ ., family = binomial, data = training_set)
summary(classifier)
#Predicting test results
prob_pred = predict(classifier, type = "response", newdata = testing_set[,1:8])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
cm = table(testing_set[, 9], y_pred)
cm
classifier = glm(formula = status ~ ., family = binomial, data = startups.rose)
summary(classifier)
#Predicting test results
prob_pred = predict(classifier, type = "response", newdata = testing_set[,1:8])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
cm = table(testing_set[, 9], y_pred)
cm
classifier = glm(formula = status ~ ., family = binomial, data = startups.smote)
summary(classifier)
#Predicting test results
prob_pred = predict(classifier, type = "response", newdata = testing_set[,1:8])
y_pred = ifelse(prob_pred > 0.5, 1, 0)
cm = table(testing_set[, 9], y_pred)
cm
classifier = svm(formula = status ~ ., data = startups.rose, type = 'C-classification', kernel = 'linear')
y_pred = predict(classifier, newdata = testing_set[-9])
confusionMatrix(testing_set[, 9], y_pred)
plot(pca$x[,1], pca$x[,2])
table(startups.rose$status)
#SMOTE sampling
#install.packages("DMwR")
library(DMwR)
table(startups.rose$status)
table(startups.smote$status)
classifier = glm(formula = status ~ ., family = binomial, data = startups.smote)
#Random Forest Classification
library(randomForest)
classifier = randomForest(x = training_set[-9], y = training_set$target, ntree = 120)
y_pred = predict(classifier, newdata = test_set[-9])
y_pred = predict(classifier, newdata = testing_set[-9])
confusionMatrix(test_set[, 9], y_pred)
classifier = randomForest(x = training_set[-9], y = training_set$target, ntree = 50)
